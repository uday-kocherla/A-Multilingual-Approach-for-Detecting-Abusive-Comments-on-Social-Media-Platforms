{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JaTEVtiGizTJ",
    "outputId": "ba653d8d-2301-478b-f6b8-e730ecd08328"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPTYr-L7e_IN"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split , StratifiedKFold\n",
    "\n",
    "\n",
    "import tensorflow as tf \n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model, load_model, save_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input,Dense, LSTM, RNN, Bidirectional, GlobalAveragePooling2D , Dropout\n",
    "\n",
    "from transformers import TFAutoModel , AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ocTygH-glL1Y",
    "outputId": "de9fe686-9f17-4daf-df3c-c81bd60e9bf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zFVpxPVfi6ju",
    "outputId": "72c93549-7468-474d-e320-7be9b0b4925e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seeded everything to seed 43\n"
     ]
    }
   ],
   "source": [
    "class config:\n",
    "  seed = 43\n",
    "  train_path = \"/content/drive/MyDrive/Social Media Mining/SMM.csv\"\n",
    "  test_path = \"/content/drive/MyDrive/Social Media Mining/SMM_test.csv\"\n",
    "  langs = {'Hindi':'hi','Telugu':'te','Marathi':'mr','Tamil':'ta','Malayalam':'ml','Bengali':'bn','Kannada':'kn','Odia':'or','Gujarati':'gu',}\n",
    "  save_dir = \"./result\"\n",
    "  AUTOTUNE = tf.data.AUTOTUNE\n",
    "  \n",
    "  #model params\n",
    "  epochs = 12\n",
    "  max_len = 64\n",
    "  batch_size = 128\n",
    "  hf_path = \"google/muril-base-cased\" \n",
    "\n",
    "def seed_everything(seed = config.seed):\n",
    "  print(f\"seeded everything to seed {seed}\")\n",
    "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "  np.random.seed(seed)\n",
    "  tf.random.set_seed(seed)\n",
    "\n",
    "if not os.path.exists(config.save_dir):\n",
    "  os.makedirs(config.save_dir)\n",
    "  \n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NkXRepiUlo9I"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(config.train_path)\n",
    "df_test = pd.read_csv(config.test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9uWjZadtmuUd",
    "outputId": "f055ec6b-b52f-4856-e58b-fc9bce16055e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./result/muril_tokenizer/tokenizer_config.json',\n",
       " './result/muril_tokenizer/special_tokens_map.json',\n",
       " './result/muril_tokenizer/vocab.txt',\n",
       " './result/muril_tokenizer/added_tokens.json',\n",
       " './result/muril_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.hf_path)\n",
    "tokenizer.save_pretrained(os.path.join(config.save_dir , \"muril_tokenizer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "py17NIT5m_6n"
   },
   "outputs": [],
   "source": [
    "def fast_encode(texts, tokenizer, chunk_size=512, maxlen=config.max_len):\n",
    "    \n",
    "    input_ids = []\n",
    "    tt_ids = []\n",
    "    at_ids = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
    "        text_chunk = texts[i:i+chunk_size]\n",
    "        encs = tokenizer(\n",
    "                    text_chunk,\n",
    "                    max_length = config.max_len,\n",
    "                    padding='max_length',\n",
    "                    truncation=True\n",
    "                    )\n",
    "        \n",
    "        input_ids.extend(encs['input_ids'])\n",
    "        tt_ids.extend(encs['token_type_ids'])\n",
    "        at_ids.extend(encs['attention_mask'])\n",
    "    \n",
    "    return {'input_ids': input_ids, 'token_type_ids': tt_ids, 'attention_mask':at_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "32309479dbee4ba68f36435449a5a02f",
      "8dc6ba706e31426b9cada96dc141b35f",
      "2a68c737886c4bc69d77d94f865b491c",
      "2935cfa3bfb948498c8fc73265df0673",
      "c0373f9ead45402ea9b5c47cfb880bb9",
      "7f9f3a6342264b50828c7208a936ab2d",
      "3e2f899387e948adad7dac78ebd9347b",
      "5806d405dff2493eb21f18e4deb27612",
      "87bb9bf7ee094838bb404e8036f579b6",
      "f70adcdc7b134d008f9b0877a2241828",
      "f7960384a4ab4a6dbcca7045a744d76c"
     ]
    },
    "id": "EB2f_sI4nUqR",
    "outputId": "dc1702d4-e882-4e50-969c-5378009e5748"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32309479dbee4ba68f36435449a5a02f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1299 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_data = fast_encode(list(df_train['commentText'].values), tokenizer)\n",
    "token_data['index'] = list(df_train['post_index'].values)\n",
    "token_data['label'] = list(df_train['label'].values)\n",
    "token_data['language'] = list(df_train['language'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5P_flLGnYn_"
   },
   "outputs": [],
   "source": [
    "df_tokenized = pd.DataFrame(token_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74lx-rK7nvn_"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_prep_function(embeddings , target):\n",
    "  input_ids = embeddings['input_ids']\n",
    "  attention_mask = embeddings['attention_mask']\n",
    "\n",
    "  target = tf.cast(target, tf.int32)\n",
    "  \n",
    "  return {'input_ids': input_ids , 'attention_mask': attention_mask}, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E_wt3GuSnzkH",
    "outputId": "1c540045-483c-496c-e71a-ff44ef00b67d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.97.213.2:8470\n",
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.97.213.2:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.97.213.2:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30E6KNeENheU"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional, BatchNormalization\n",
    "from keras.layers import Flatten, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d__EG8_jn3WU"
   },
   "outputs": [],
   "source": [
    "def create_model(transformer_model):\n",
    "  input_id_layer = Input(shape=(config.max_len,) ,dtype = tf.int32 , name = 'input_ids')\n",
    "  attention_mask_layer = Input(shape=(config.max_len,) , dtype = tf.int32 , name = 'attention_mask')\n",
    "\n",
    "  transformer = transformer_model(input_ids = input_id_layer , attention_mask = attention_mask_layer)[0]\n",
    "  transformer_output = transformer[:,0,:]\n",
    "\n",
    "  bd_layer = BatchNormalization()(transformer)\n",
    "  x = Dropout(0.1)(bd_layer)\n",
    "  x1 = tf.keras.layers.Conv1D(1,1)(x)\n",
    "  x1 = tf.keras.layers.Flatten()(x1)\n",
    "  bd_layer_1 = BatchNormalization()(x1)\n",
    "  predictions = Dense(1, activation = \"sigmoid\")(bd_layer_1)\n",
    "\n",
    "  model = Model(inputs=[input_id_layer , attention_mask_layer], outputs = predictions)\n",
    "  model.compile(\n",
    "      optimizer = Adam(learning_rate= 1e-5),\n",
    "      metrics = 'accuracy',\n",
    "      loss = 'binary_crossentropy'\n",
    "  )\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850,
     "referenced_widgets": [
      "43bc3ba0dcf8488081e92f7296edd022",
      "0777313fb072423d84757dacb72e25fb",
      "6af54ce01cd649d0a82699f13a3258e1",
      "4188ad3931e04d3ab2a08b98356aa39a",
      "5904b68e8d1d49f09f0de1a8a064d0ff",
      "d9b303506a5045e2be0329a44d8bd74e",
      "a5208b51eaf7452c90bc00031e2e8622",
      "2e1b6ea4ce084b309c361dcdcd9b92d5",
      "e33464a55d234493ae054048ee0e505d",
      "66c8fc92eef742c8b17c392d72208a9d",
      "26588fdfa68542a1ae78b9db0bf7c24c"
     ]
    },
    "id": "FUz1DFpqoOxx",
    "outputId": "ea4060df-b374-46a8-9d0f-f594bba84b08"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43bc3ba0dcf8488081e92f7296edd022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.45G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/muril-base-cased were not used when initializing TFBertModel: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertModel were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['bert/pooler/dense/kernel:0', 'bert/pooler/dense/bias:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  237556224   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 64,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 64, 768)     3072        ['tf_bert_model[0][0]']          \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 64, 768)      0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 64, 1)        769         ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 64)           0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 64)          256         ['flatten[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            65          ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 237,560,386\n",
      "Trainable params: 237,558,722\n",
      "Non-trainable params: 1,664\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "  transformer_model = TFAutoModel.from_pretrained(config.hf_path)\n",
    "  model = create_model(transformer_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Fe44ybecQGkp",
    "outputId": "a3a91cc4-7388-4c06-f8cf-cbd694996158"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[104, 10478, 14318, 2254, 1115, 105, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>238566</td>\n",
       "      <td>0</td>\n",
       "      <td>Hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[104, 116969, 101565, 179, 1113, 48907, 1206, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>7009</td>\n",
       "      <td>0</td>\n",
       "      <td>Hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[104, 87541, 9535, 94108, 1278, 1274, 14604, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>404648</td>\n",
       "      <td>1</td>\n",
       "      <td>Hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[104, 7313, 100, 4430, 1159, 3032, 95948, 2003...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>5057</td>\n",
       "      <td>0</td>\n",
       "      <td>Hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[104, 9480, 6985, 81441, 2003, 25124, 60885, 9...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>107146</td>\n",
       "      <td>1</td>\n",
       "      <td>Hindi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  ... language\n",
       "0  [104, 10478, 14318, 2254, 1115, 105, 0, 0, 0, ...  ...    Hindi\n",
       "1  [104, 116969, 101565, 179, 1113, 48907, 1206, ...  ...    Hindi\n",
       "2  [104, 87541, 9535, 94108, 1278, 1274, 14604, 1...  ...    Hindi\n",
       "3  [104, 7313, 100, 4430, 1159, 3032, 95948, 2003...  ...    Hindi\n",
       "4  [104, 9480, 6985, 81441, 2003, 25124, 60885, 9...  ...    Hindi\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokenized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWifYcajQPQP"
   },
   "outputs": [],
   "source": [
    "df_tokenized_onelang = df_tokenized[df_tokenized['language'] == 'Hindi']\n",
    "df_tokenized_onelang_label = df_tokenized_onelang['label']\n",
    "df_tokenized_onelang = df_tokenized_onelang.drop(columns=['label'])\n",
    "df_tokenized_alllang = df_tokenized[df_tokenized['language'] != 'Hindi']\n",
    "df_tokenized_alllang_label = df_tokenized_alllang['label']\n",
    "df_tokenized_alllang = df_tokenized_alllang.drop(columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBVO0kdX-W2s"
   },
   "outputs": [],
   "source": [
    "df_tokenized_onelang_t = df_tokenized[df_tokenized['language'] == 'Telugu']\n",
    "df_tokenized_onelang_t_label = df_tokenized_onelang_t['label']\n",
    "df_tokenized_onelang_t = df_tokenized_onelang_t.drop(columns=['label'])\n",
    "df_tokenized_alllang_t = df_tokenized[df_tokenized['language'] != 'Telugu']\n",
    "df_tokenized_alllang_t_label = df_tokenized_alllang_t['label']\n",
    "df_tokenized_alllang_t = df_tokenized_alllang_t.drop(columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNWdGQ37kkW-"
   },
   "outputs": [],
   "source": [
    "df_tokenized_onelang_a = df_tokenized[df_tokenized['language'] == 'Malayalam']\n",
    "df_tokenized_onelang_a_label = df_tokenized_onelang_a['label']\n",
    "df_tokenized_onelang_a = df_tokenized_onelang_a.drop(columns=['label'])\n",
    "df_tokenized_alllang_a = df_tokenized[df_tokenized['language'] != 'Malayalam']\n",
    "df_tokenized_alllang_a_label = df_tokenized_alllang_a['label']\n",
    "df_tokenized_alllang_a = df_tokenized_alllang_a.drop(columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJDq6nvErqAF"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train_onelang,x_val_onelang,y_train_onelang,y_val_onelang=train_test_split(df_tokenized_onelang, df_tokenized_onelang_label, test_size=0.1,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADwnT6noAHOh"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train_onelang_t,x_val_onelang_t,y_train_onelang_t,y_val_onelang_t=train_test_split(df_tokenized_onelang_t, df_tokenized_onelang_t_label, test_size=0.1,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bFwMdMjlJb3"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train_onelang_a,x_val_onelang_a,y_train_onelang_a,y_val_onelang_a=train_test_split(df_tokenized_onelang_a, df_tokenized_onelang_a_label, test_size=0.1,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_W3e1ZtRXyDX"
   },
   "outputs": [],
   "source": [
    "x_train_alllang = df_tokenized_alllang.append(x_train_onelang)\n",
    "y_train_alllang = df_tokenized_alllang_label.append(y_train_onelang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZvIS69EcAk8Y"
   },
   "outputs": [],
   "source": [
    "x_train_alllang_t = df_tokenized_alllang_t.append(x_train_onelang_t)\n",
    "y_train_alllang_t = df_tokenized_alllang_t_label.append(y_train_onelang_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dppeV53Lk6V5"
   },
   "outputs": [],
   "source": [
    "x_train_alllang_a = df_tokenized_alllang_a.append(x_train_onelang_a)\n",
    "y_train_alllang_a = df_tokenized_alllang_a_label.append(y_train_onelang_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VEm-cqBai7cg"
   },
   "outputs": [],
   "source": [
    "test_embeddings = {'input_ids': x_val_onelang['input_ids'].tolist() ,\"attention_mask\":x_val_onelang['attention_mask'].tolist()}\n",
    "test_data = tf.data.Dataset.from_tensor_slices((test_embeddings, y_val_onelang))\n",
    "test_data = (\n",
    "    test_data\n",
    "    .map(train_prep_function , num_parallel_calls = config.AUTOTUNE)\n",
    "    .batch(config.batch_size)\n",
    "    .prefetch(config.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3noCOk9rA211"
   },
   "outputs": [],
   "source": [
    "test_embeddings_t = {'input_ids': x_val_onelang_t['input_ids'].tolist() ,\"attention_mask\":x_val_onelang_t['attention_mask'].tolist()}\n",
    "test_data_t = tf.data.Dataset.from_tensor_slices((test_embeddings_t, y_val_onelang_t))\n",
    "test_data_t = (\n",
    "    test_data_t\n",
    "    .map(train_prep_function , num_parallel_calls = config.AUTOTUNE)\n",
    "    .batch(config.batch_size)\n",
    "    .prefetch(config.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMGzy-SolYqG"
   },
   "outputs": [],
   "source": [
    "test_embeddings_a = {'input_ids': x_val_onelang_a['input_ids'].tolist() ,\"attention_mask\":x_val_onelang_a['attention_mask'].tolist()}\n",
    "test_data_a = tf.data.Dataset.from_tensor_slices((test_embeddings_a, y_val_onelang_a))\n",
    "test_data_a = (\n",
    "    test_data_a\n",
    "    .map(train_prep_function , num_parallel_calls = config.AUTOTUNE)\n",
    "    .batch(config.batch_size)\n",
    "    .prefetch(config.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "yxizHMwuoRml",
    "outputId": "5f3e6edd-2e7a-4817-dc32-c281ddc49ea6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training steps 1238 , validation steps 239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/muril-base-cased were not used when initializing TFBertModel: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertModel were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['bert/pooler/dense/kernel:0', 'bert/pooler/dense/bias:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'IteratorGetNext:0' shape=(None, 64) dtype=int32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 64) dtype=int32>, <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=int32>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:443: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 151514880 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2970: StrategyBase.unwrap (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "use `experimental_local_results` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2970: StrategyBase.unwrap (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "use `experimental_local_results` instead.\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'IteratorGetNext:0' shape=(None, 64) dtype=int32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 64) dtype=int32>, <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=int32>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1238/1238 [==============================] - ETA: 0s - loss: 0.4895 - accuracy: 0.7643"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 64) dtype=int32>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 64) dtype=int32>, <tf.Tensor 'cond/Identity_16:0' shape=(None,) dtype=int32>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1238/1238 [==============================] - 255s 142ms/step - loss: 0.4895 - accuracy: 0.7643 - val_loss: 0.5237 - val_accuracy: 0.7538\n",
      "Epoch 2/12\n",
      "1238/1238 [==============================] - 166s 134ms/step - loss: 0.3845 - accuracy: 0.8416 - val_loss: 0.4386 - val_accuracy: 0.8024\n",
      "Epoch 3/12\n",
      "1238/1238 [==============================] - 166s 134ms/step - loss: 0.3944 - accuracy: 0.8356 - val_loss: 0.3758 - val_accuracy: 0.8479\n",
      "Epoch 4/12\n",
      "1238/1238 [==============================] - 167s 135ms/step - loss: 0.3709 - accuracy: 0.8503 - val_loss: 0.3604 - val_accuracy: 0.8513\n",
      "Epoch 5/12\n",
      "1238/1238 [==============================] - 166s 134ms/step - loss: 0.3510 - accuracy: 0.8596 - val_loss: 0.3759 - val_accuracy: 0.8387\n",
      "Epoch 6/12\n",
      "1238/1238 [==============================] - 166s 134ms/step - loss: 0.3368 - accuracy: 0.8674 - val_loss: 0.3657 - val_accuracy: 0.8481\n",
      "Epoch 7/12\n",
      "1238/1238 [==============================] - 166s 134ms/step - loss: 0.3520 - accuracy: 0.8593 - val_loss: 0.3478 - val_accuracy: 0.8600\n",
      "Epoch 8/12\n",
      "1238/1238 [==============================] - 167s 135ms/step - loss: 0.3466 - accuracy: 0.8615 - val_loss: 0.3493 - val_accuracy: 0.8590\n",
      "Epoch 9/12\n",
      "1238/1238 [==============================] - 166s 134ms/step - loss: 0.3207 - accuracy: 0.8746 - val_loss: 0.3746 - val_accuracy: 0.8443\n",
      "Epoch 10/12\n",
      "1238/1238 [==============================] - 166s 134ms/step - loss: 0.3163 - accuracy: 0.8772 - val_loss: 0.3613 - val_accuracy: 0.8525\n",
      "Epoch 11/12\n",
      "1238/1238 [==============================] - 167s 135ms/step - loss: 0.3331 - accuracy: 0.8677 - val_loss: 0.3454 - val_accuracy: 0.8636\n",
      "Epoch 12/12\n",
      "1238/1238 [==============================] - 167s 134ms/step - loss: 0.3321 - accuracy: 0.8684 - val_loss: 0.3405 - val_accuracy: 0.8638\n"
     ]
    }
   ],
   "source": [
    "train_embeddings = {'input_ids': x_train_alllang['input_ids'].tolist() ,\"attention_mask\":x_train_alllang['attention_mask'].tolist()}\n",
    "train_steps = len(train_embeddings['input_ids'])//config.batch_size//4\n",
    "validation_steps = len(test_embeddings['input_ids'])//config.batch_size\n",
    "print(f\"training steps {train_steps} , validation steps {validation_steps}\")\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_embeddings , y_train_alllang))\n",
    "train_dataset = (\n",
    "    train_dataset\n",
    "    .shuffle(1024*2)\n",
    "    .map(train_prep_function , num_parallel_calls = config.AUTOTUNE)\n",
    "    .repeat()\n",
    "    .batch(config.batch_size)\n",
    "    .prefetch(config.AUTOTUNE)\n",
    ")\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_embeddings , y_val_onelang))\n",
    "test_dataset = (\n",
    "    test_dataset\n",
    "    .map(train_prep_function , num_parallel_calls = config.AUTOTUNE)\n",
    "    .batch(config.batch_size)\n",
    "    .prefetch(config.AUTOTUNE)\n",
    ")\n",
    "with strategy.scope():\n",
    "  transformer_model = TFAutoModel.from_pretrained(config.hf_path)\n",
    "  model = create_model(transformer_model)\n",
    "hist = model.fit(train_dataset,steps_per_epoch= train_steps,validation_data= test_dataset, epochs = config.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "suL4J6GAlY7S"
   },
   "outputs": [],
   "source": [
    "y_orig_alllang = model.predict(test_data , verbose = 1).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TbHnvJoB1EWd"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print('Classification report for Hindi after training on all languages')\n",
    "print(classification_report(y_orig_alllang, y_val_onelang, target_names=['Non-Abusive','Abusive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 937,
     "referenced_widgets": [
      "0adce5a1751e4e8888dd9063f38fa7ac",
      "512d140980954df984f5fa187daba712",
      "77b02d6768464879bf052c010d8bc099",
      "d58ede2be6404684aaa9ac8cc646b288",
      "2af651603e4744ff9871e17f892f91a6",
      "0bcb113152d94c28af8c1b85a29f5bd2",
      "aac1575aa2084411ba026ccfa562ceaf",
      "6abf715d52a844a3a738e481d3eeda73",
      "f287437bceb44a2d96c3ff0fbd5ac2f2",
      "a9b3ee8b2a984fb9a8771b916cd639e1",
      "f0926b2df7e349b6b08a9b787485c026"
     ]
    },
    "id": "aTNNzoFMfOgg",
    "outputId": "4886943e-ad6d-45a0-c5f1-62e234ead6e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training steps 539 , validation steps 239\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0adce5a1751e4e8888dd9063f38fa7ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.45G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/muril-base-cased were not used when initializing TFBertModel: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertModel were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['bert/pooler/dense/bias:0', 'bert/pooler/dense/kernel:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'IteratorGetNext:0' shape=(None, 64) dtype=int32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 64) dtype=int32>, <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=int32>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:443: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 151514880 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2970: StrategyBase.unwrap (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "use `experimental_local_results` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2970: StrategyBase.unwrap (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "use `experimental_local_results` instead.\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'IteratorGetNext:0' shape=(None, 64) dtype=int32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 64) dtype=int32>, <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=int32>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539/539 [==============================] - ETA: 0s - loss: 0.5251 - accuracy: 0.7477"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 64) dtype=int32>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 64) dtype=int32>, <tf.Tensor 'cond/Identity_16:0' shape=(None,) dtype=int32>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539/539 [==============================] - 162s 155ms/step - loss: 0.5251 - accuracy: 0.7477 - val_loss: 0.4692 - val_accuracy: 0.8267\n",
      "Epoch 2/12\n",
      "539/539 [==============================] - 75s 139ms/step - loss: 0.4128 - accuracy: 0.8266 - val_loss: 0.3702 - val_accuracy: 0.8468\n",
      "Epoch 3/12\n",
      "539/539 [==============================] - 75s 139ms/step - loss: 0.3881 - accuracy: 0.8414 - val_loss: 0.3618 - val_accuracy: 0.8521\n",
      "Epoch 4/12\n",
      "167/539 [========>.....................] - ETA: 46s - loss: 0.3784 - accuracy: 0.8446"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-8e33ebbada16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mtransformer_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_onelang\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_steps_onelang\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1219\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    434\u001b[0m     \"\"\"\n\u001b[1;32m    435\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m       raise ValueError(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mndim\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   3073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0marray_function_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ndim_dispatcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3076\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m     \"\"\"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_embeddings_onelang = {'input_ids': x_train_onelang['input_ids'].tolist() ,\"attention_mask\":x_train_onelang['attention_mask'].tolist()}\n",
    "train_steps_onelang = len(train_embeddings_onelang['input_ids'])//config.batch_size//4\n",
    "validation_steps= len(test_embeddings['input_ids'])//config.batch_size\n",
    "print(f\"training steps {train_steps_onelang} , validation steps {validation_steps}\")\n",
    "train_dataset_onelang = tf.data.Dataset.from_tensor_slices((train_embeddings_onelang , y_train_onelang))\n",
    "train_dataset_onelang = (\n",
    "    train_dataset_onelang\n",
    "    .shuffle(1024*2)\n",
    "    .map(train_prep_function , num_parallel_calls = config.AUTOTUNE)\n",
    "    .repeat()\n",
    "    .batch(config.batch_size)\n",
    "    .prefetch(config.AUTOTUNE)\n",
    ")\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_embeddings , y_val_onelang))\n",
    "test_dataset = (\n",
    "    test_dataset\n",
    "    .map(train_prep_function , num_parallel_calls = config.AUTOTUNE)\n",
    "    .batch(config.batch_size)\n",
    "    .prefetch(config.AUTOTUNE)\n",
    ")\n",
    "with strategy.scope():\n",
    "  transformer_model = TFAutoModel.from_pretrained(config.hf_path)\n",
    "  model1 = create_model(transformer_model)\n",
    "hist = model1.fit(train_dataset_onelang,steps_per_epoch= train_steps_onelang,validation_data= test_dataset, epochs = config.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qejxa8Bw31Vq"
   },
   "outputs": [],
   "source": [
    "y_orig_onelang = model.predict(test_data , verbose = 1).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9ELagk_4BGB"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print('Classification report for Hindi after training on Hindi language')\n",
    "print(classification_report(y_orig_onelang, y_val_onelang, target_names=['Non-Abusive','Abusive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VvsYApNxBHnA"
   },
   "outputs": [],
   "source": [
    "train_embeddings_t = {'input_ids': x_train_alllang_t['input_ids'].tolist() ,\"attention_mask\":x_train_alllang_t['attention_mask'].tolist()}\n",
    "train_steps_t = len(train_embeddings_t['input_ids'])//config.batch_size//4\n",
    "validation_steps_t = len(test_embeddings_t['input_ids'])//config.batch_size\n",
    "print(f\"training steps {train_steps_t} , validation steps {validation_steps_t}\")\n",
    "train_dataset_t = tf.data.Dataset.from_tensor_slices((train_embeddings_t , y_train_alllang_t))\n",
    "train_dataset_t = (\n",
    "    train_dataset_t\n",
    "    .shuffle(1024*2)\n",
    "    .map(train_prep_function , num_parallel_calls = config.AUTOTUNE)\n",
    "    .repeat()\n",
    "    .batch(config.batch_size)\n",
    "    .prefetch(config.AUTOTUNE)\n",
    ")\n",
    "test_dataset_t = tf.data.Dataset.from_tensor_slices((test_embeddings_t , y_val_onelang_t))\n",
    "test_dataset_t = (\n",
    "    test_dataset_t\n",
    "    .map(train_prep_function , num_parallel_calls = config.AUTOTUNE)\n",
    "    .batch(config.batch_size)\n",
    "    .prefetch(config.AUTOTUNE)\n",
    ")\n",
    "with strategy.scope():\n",
    "  transformer_model = TFAutoModel.from_pretrained(config.hf_path)\n",
    "  model = create_model(transformer_model)\n",
    "hist = model.fit(train_dataset_t,steps_per_epoch= train_steps_t,validation_data= test_dataset_t, epochs = config.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "syzDEKQ5BoF7"
   },
   "outputs": [],
   "source": [
    "y_orig_alllang_t = model.predict(test_data_t , verbose = 1).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "co8tnhtACIBD"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print('Classification report for Telugu after training on all languages')\n",
    "print(classification_report(y_orig_alllang_t, y_val_onelang_t, target_names=['Non-Abusive','Abusive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9DyZ9YJCeJ-"
   },
   "outputs": [],
   "source": [
    "train_embeddings_onelang_t = {'input_ids': x_train_onelang_t['input_ids'].tolist() ,\"attention_mask\":x_train_onelang_t['attention_mask'].tolist()}\n",
    "train_steps_onelang_t = len(train_embeddings_onelang_t['input_ids'])//config.batch_size//4\n",
    "validation_steps_t= len(test_embeddings_t['input_ids'])//config.batch_size\n",
    "print(f\"training steps {train_steps_onelang_t} , validation steps {validation_steps_t}\")\n",
    "train_dataset_onelang_t = tf.data.Dataset.from_tensor_slices((train_embeddings_onelang_t , y_train_onelang_t))\n",
    "train_dataset_onelang_t = (\n",
    "    train_dataset_onelang_t\n",
    "    .shuffle(1024*2)\n",
    "    .map(train_prep_function , num_parallel_calls = config.AUTOTUNE)\n",
    "    .repeat()\n",
    "    .batch(config.batch_size)\n",
    "    .prefetch(config.AUTOTUNE)\n",
    ")\n",
    "test_dataset_t = tf.data.Dataset.from_tensor_slices((test_embeddings_t , y_val_onelang_t))\n",
    "test_dataset_t = (\n",
    "    test_dataset_t\n",
    "    .map(train_prep_function , num_parallel_calls = config.AUTOTUNE)\n",
    "    .batch(config.batch_size)\n",
    "    .prefetch(config.AUTOTUNE)\n",
    ")\n",
    "with strategy.scope():\n",
    "  transformer_model = TFAutoModel.from_pretrained(config.hf_path)\n",
    "  model1 = create_model(transformer_model)\n",
    "hist = model.fit(train_dataset_onelang_t,steps_per_epoch= train_steps_onelang_t,validation_data= test_dataset_t, epochs = config.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8B9GXhT8EKhV"
   },
   "outputs": [],
   "source": [
    "y_orig_onelang_t = model.predict(test_data_t , verbose = 1).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4BfO1XiEOl5"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print('Classification report for Telugu after training on Telugu language')\n",
    "print(classification_report(y_orig_onelang_t, y_val_onelang_t, target_names=['Non-Abusive','Abusive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_DQucQWelzE6",
    "outputId": "f87a0791-5393-46a9-9d04-5af9e747c3be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training steps 1290 , validation steps 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/muril-base-cased were not used when initializing TFBertModel: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertModel were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['bert/pooler/dense/bias:0', 'bert/pooler/dense/kernel:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'IteratorGetNext:0' shape=(None, 64) dtype=int32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 64) dtype=int32>, <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=int32>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:443: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 151514880 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'IteratorGetNext:0' shape=(None, 64) dtype=int32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 64) dtype=int32>, <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=int32>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1290/1290 [==============================] - ETA: 0s - loss: 0.5252 - accuracy: 0.7359"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 64) dtype=int32>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 64) dtype=int32>, <tf.Tensor 'cond/Identity_16:0' shape=(None,) dtype=int32>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1290/1290 [==============================] - 271s 149ms/step - loss: 0.5252 - accuracy: 0.7359 - val_loss: 0.5663 - val_accuracy: 0.7610\n",
      "Epoch 2/12\n",
      "1290/1290 [==============================] - 178s 138ms/step - loss: 0.4058 - accuracy: 0.8291 - val_loss: 0.5735 - val_accuracy: 0.7213\n",
      "Epoch 3/12\n",
      "1290/1290 [==============================] - 180s 140ms/step - loss: 0.3844 - accuracy: 0.8423 - val_loss: 0.6021 - val_accuracy: 0.7610\n",
      "Epoch 4/12\n",
      "1290/1290 [==============================] - 184s 143ms/step - loss: 0.3939 - accuracy: 0.8446 - val_loss: 0.4496 - val_accuracy: 0.8843\n",
      "Epoch 5/12\n",
      "1290/1290 [==============================] - 181s 141ms/step - loss: 0.3653 - accuracy: 0.8534 - val_loss: 0.3844 - val_accuracy: 0.8213\n",
      "Epoch 6/12\n",
      "1290/1290 [==============================] - 181s 141ms/step - loss: 0.3514 - accuracy: 0.8596 - val_loss: 0.3803 - val_accuracy: 0.8453\n",
      "Epoch 7/12\n",
      "1290/1290 [==============================] - 181s 140ms/step - loss: 0.3468 - accuracy: 0.8614 - val_loss: 0.3925 - val_accuracy: 0.8311\n",
      "Epoch 8/12\n",
      "1290/1290 [==============================] - 181s 141ms/step - loss: 0.3525 - accuracy: 0.8658 - val_loss: 0.4378 - val_accuracy: 0.8887\n",
      "Epoch 9/12\n",
      "1290/1290 [==============================] - 182s 141ms/step - loss: 0.3395 - accuracy: 0.8669 - val_loss: 0.3410 - val_accuracy: 0.8601\n",
      "Epoch 10/12\n",
      "1290/1290 [==============================] - 181s 140ms/step - loss: 0.3267 - accuracy: 0.8718 - val_loss: 0.3423 - val_accuracy: 0.8738\n",
      "Epoch 11/12\n",
      "1290/1290 [==============================] - 180s 140ms/step - loss: 0.3263 - accuracy: 0.8713 - val_loss: 0.3527 - val_accuracy: 0.8670\n",
      "Epoch 12/12\n",
      "1290/1290 [==============================] - 180s 140ms/step - loss: 0.3309 - accuracy: 0.8750 - val_loss: 0.3765 - val_accuracy: 0.8975\n"
     ]
    }
   ],
   "source": [
    "train_embeddings_a = {'input_ids': x_train_alllang_a['input_ids'].tolist() ,\"attention_mask\":x_train_alllang_a['attention_mask'].tolist()}\n",
    "train_steps_a = len(train_embeddings_a['input_ids'])//config.batch_size//4\n",
    "validation_steps_a = len(test_embeddings_a['input_ids'])//config.batch_size\n",
    "print(f\"training steps {train_steps_a} , validation steps {validation_steps_a}\")\n",
    "train_dataset_a = tf.data.Dataset.from_tensor_slices((train_embeddings_a , y_train_alllang_a))\n",
    "train_dataset_a = (\n",
    "    train_dataset_a\n",
    "    .shuffle(1024*2)\n",
    "    .map(train_prep_function , num_parallel_calls = config.AUTOTUNE)\n",
    "    .repeat()\n",
    "    .batch(config.batch_size)\n",
    "    .prefetch(config.AUTOTUNE)\n",
    ")\n",
    "test_dataset_a = tf.data.Dataset.from_tensor_slices((test_embeddings_a , y_val_onelang_a))\n",
    "test_dataset_a = (\n",
    "    test_dataset_a\n",
    "    .map(train_prep_function , num_parallel_calls = config.AUTOTUNE)\n",
    "    .batch(config.batch_size)\n",
    "    .prefetch(config.AUTOTUNE)\n",
    ")\n",
    "with strategy.scope():\n",
    "  transformer_model = TFAutoModel.from_pretrained(config.hf_path)\n",
    "  model_a_all = create_model(transformer_model)\n",
    "hist = model_a_all.fit(train_dataset_a,steps_per_epoch= train_steps_a,validation_data= test_dataset_a, epochs = config.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cJMK7DpOmQnU",
    "outputId": "4574c943-0d61-424b-bc76-2e6ec14dc900"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond/Identity:0' shape=(None, 64) dtype=int32>, <tf.Tensor 'cond/Identity_8:0' shape=(None, 64) dtype=int32>, <tf.Tensor 'cond/Identity_16:0' shape=(None,) dtype=int32>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 18s 333ms/step\n"
     ]
    }
   ],
   "source": [
    "y_orig_alllang_a = model_a_all.predict(test_data_a , verbose = 1).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4kMNRM9QmX45",
    "outputId": "4354bfb9-8536-41fa-fdc6-a8dd4dffde5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Malayalam after training on all languages\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-Abusive       0.93      0.94      0.93      3121\n",
      "     Abusive       0.79      0.78      0.78       976\n",
      "\n",
      "    accuracy                           0.90      4097\n",
      "   macro avg       0.86      0.86      0.86      4097\n",
      "weighted avg       0.90      0.90      0.90      4097\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print('Classification report for Malayalam after training on all languages')\n",
    "print(classification_report(y_orig_alllang_a, y_val_onelang_a, target_names=['Non-Abusive','Abusive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mh_nhdQVmoME",
    "outputId": "a0e759c5-5447-4528-d831-e1c9a385a5f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training steps 72 , validation steps 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/muril-base-cased were not used when initializing TFBertModel: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertModel were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['bert/pooler/dense/bias:0', 'bert/pooler/dense/kernel:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/embeddings/word_embeddings/weight:0', 'tf_bert_model/bert/embeddings/token_type_embeddings/embeddings:0', 'tf_bert_model/bert/embeddings/position_embeddings/embeddings:0', 'tf_bert_model/bert/embeddings/LayerNorm/gamma:0', 'tf_bert_model/bert/embeddings/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._0/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._0/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._0/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._0/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._0/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._0/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._0/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._0/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._0/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._0/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._1/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._1/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._1/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._1/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._1/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._1/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._1/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._1/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._1/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._1/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._2/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._2/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._2/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._2/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._2/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._2/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._2/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._2/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._2/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._2/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._3/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._3/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._3/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._3/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._3/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._3/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._3/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._3/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._3/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._3/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._4/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._4/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._4/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._4/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._4/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._4/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._4/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._4/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._4/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._4/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._5/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._5/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._5/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._5/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._5/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._5/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._5/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._5/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._5/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._5/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._6/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._6/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._6/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._6/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._6/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._6/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._6/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._6/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._6/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._6/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._7/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._7/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._7/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._7/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._7/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._7/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._7/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._7/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._7/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._7/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._8/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._8/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._8/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._8/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._8/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._8/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._8/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._8/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._8/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._8/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._9/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._9/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._9/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._9/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._9/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._9/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._9/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._9/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._9/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._9/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/embeddings/word_embeddings/weight:0', 'tf_bert_model/bert/embeddings/token_type_embeddings/embeddings:0', 'tf_bert_model/bert/embeddings/position_embeddings/embeddings:0', 'tf_bert_model/bert/embeddings/LayerNorm/gamma:0', 'tf_bert_model/bert/embeddings/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._0/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._0/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._0/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._0/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._0/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._0/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._0/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._0/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._0/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._0/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._1/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._1/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._1/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._1/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._1/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._1/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._1/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._1/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._1/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._1/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._2/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._2/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._2/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._2/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._2/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._2/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._2/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._2/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._2/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._2/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._3/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._3/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._3/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._3/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._3/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._3/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._3/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._3/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._3/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._3/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._4/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._4/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._4/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._4/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._4/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._4/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._4/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._4/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._4/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._4/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._5/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._5/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._5/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._5/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._5/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._5/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._5/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._5/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._5/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._5/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._6/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._6/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._6/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._6/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._6/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._6/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._6/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._6/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._6/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._6/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._7/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._7/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._7/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._7/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._7/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._7/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._7/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._7/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._7/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._7/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._8/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._8/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._8/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._8/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._8/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._8/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._8/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._8/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._8/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._8/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._9/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._9/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._9/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._9/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._9/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._9/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._9/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._9/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._9/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._9/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/embeddings/word_embeddings/weight:0', 'tf_bert_model/bert/embeddings/token_type_embeddings/embeddings:0', 'tf_bert_model/bert/embeddings/position_embeddings/embeddings:0', 'tf_bert_model/bert/embeddings/LayerNorm/gamma:0', 'tf_bert_model/bert/embeddings/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._0/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._0/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._0/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._0/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._0/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._0/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._0/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._0/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._0/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._0/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._1/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._1/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._1/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._1/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._1/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._1/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._1/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._1/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._1/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._1/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._2/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._2/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._2/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._2/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._2/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._2/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._2/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._2/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._2/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._2/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._3/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._3/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._3/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._3/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._3/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._3/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._3/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._3/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._3/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._3/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._4/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._4/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._4/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._4/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._4/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._4/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._4/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._4/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._4/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._4/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._5/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._5/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._5/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._5/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._5/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._5/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._5/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._5/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._5/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._5/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._6/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._6/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._6/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._6/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._6/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._6/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._6/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._6/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._6/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._6/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._7/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._7/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._7/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._7/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._7/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._7/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._7/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._7/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._7/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._7/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._8/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._8/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._8/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._8/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._8/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._8/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._8/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._8/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._8/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._8/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._9/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._9/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._9/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._9/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._9/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._9/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._9/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._9/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._9/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._9/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/embeddings/word_embeddings/weight:0', 'tf_bert_model/bert/embeddings/token_type_embeddings/embeddings:0', 'tf_bert_model/bert/embeddings/position_embeddings/embeddings:0', 'tf_bert_model/bert/embeddings/LayerNorm/gamma:0', 'tf_bert_model/bert/embeddings/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._0/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._0/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._0/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._0/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._0/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._0/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._0/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._0/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._0/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._0/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._1/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._1/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._1/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._1/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._1/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._1/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._1/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._1/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._1/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._1/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._2/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._2/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._2/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._2/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._2/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._2/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._2/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._2/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._2/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._2/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._3/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._3/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._3/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._3/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._3/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._3/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._3/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._3/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._3/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._3/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._4/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._4/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._4/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._4/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._4/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._4/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._4/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._4/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._4/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._4/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._5/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._5/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._5/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._5/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._5/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._5/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._5/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._5/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._5/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._5/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._6/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._6/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._6/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._6/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._6/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._6/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._6/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._6/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._6/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._6/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._7/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._7/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._7/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._7/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._7/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._7/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._7/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._7/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._7/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._7/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._8/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._8/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._8/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._8/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._8/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._8/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._8/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._8/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._8/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._8/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._9/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._9/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._9/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._9/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._9/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._9/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._9/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._9/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._9/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._9/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_model/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_model/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 213s 3s/step - loss: 0.8031 - accuracy: 0.4901 - val_loss: 0.6933 - val_accuracy: 0.4857\n",
      "Epoch 2/12\n",
      "72/72 [==============================] - 184s 3s/step - loss: 0.8005 - accuracy: 0.4919 - val_loss: 0.6909 - val_accuracy: 0.5953\n",
      "Epoch 3/12\n",
      "72/72 [==============================] - 184s 3s/step - loss: 0.7896 - accuracy: 0.4974 - val_loss: 0.6874 - val_accuracy: 0.6322\n",
      "Epoch 4/12\n",
      "72/72 [==============================] - 183s 3s/step - loss: 0.7828 - accuracy: 0.4996 - val_loss: 0.6837 - val_accuracy: 0.6297\n",
      "Epoch 5/12\n",
      "72/72 [==============================] - 180s 3s/step - loss: 0.7772 - accuracy: 0.5065 - val_loss: 0.6778 - val_accuracy: 0.6356\n",
      "Epoch 6/12\n",
      "72/72 [==============================] - 186s 3s/step - loss: 0.7721 - accuracy: 0.5085 - val_loss: 0.6721 - val_accuracy: 0.6439\n",
      "Epoch 7/12\n",
      "72/72 [==============================] - 182s 3s/step - loss: 0.7660 - accuracy: 0.5129 - val_loss: 0.6708 - val_accuracy: 0.6290\n",
      "Epoch 8/12\n",
      "72/72 [==============================] - 179s 2s/step - loss: 0.7589 - accuracy: 0.5123 - val_loss: 0.6748 - val_accuracy: 0.6029\n",
      "Epoch 9/12\n",
      "72/72 [==============================] - 181s 3s/step - loss: 0.7462 - accuracy: 0.5233 - val_loss: 0.6837 - val_accuracy: 0.5858\n",
      "Epoch 10/12\n",
      "72/72 [==============================] - 185s 3s/step - loss: 0.7407 - accuracy: 0.5254 - val_loss: 0.6926 - val_accuracy: 0.5863\n",
      "Epoch 11/12\n",
      "72/72 [==============================] - 179s 2s/step - loss: 0.7415 - accuracy: 0.5298 - val_loss: 0.6973 - val_accuracy: 0.5860\n",
      "Epoch 12/12\n",
      "72/72 [==============================] - 172s 2s/step - loss: 0.7301 - accuracy: 0.5432 - val_loss: 0.7000 - val_accuracy: 0.5838\n"
     ]
    }
   ],
   "source": [
    "train_embeddings_onelang_a = {'input_ids': x_train_onelang_a['input_ids'].tolist() ,\"attention_mask\":x_train_onelang_a['attention_mask'].tolist()}\n",
    "train_steps_onelang_a = len(train_embeddings_onelang_a['input_ids'])//config.batch_size//4\n",
    "validation_steps_a= len(test_embeddings_a['input_ids'])//config.batch_size\n",
    "print(f\"training steps {train_steps_onelang_a} , validation steps {validation_steps_a}\")\n",
    "train_dataset_onelang_a = tf.data.Dataset.from_tensor_slices((train_embeddings_onelang_a , y_train_onelang_a))\n",
    "train_dataset_onelang_a = (\n",
    "    train_dataset_onelang_a\n",
    "    .shuffle(1024*2)\n",
    "    .map(train_prep_function , num_parallel_calls = config.AUTOTUNE)\n",
    "    .repeat()\n",
    "    .batch(config.batch_size)\n",
    "    .prefetch(config.AUTOTUNE)\n",
    ")\n",
    "test_dataset_a = tf.data.Dataset.from_tensor_slices((test_embeddings_a , y_val_onelang_a))\n",
    "test_dataset_a = (\n",
    "    test_dataset_a\n",
    "    .map(train_prep_function , num_parallel_calls = config.AUTOTUNE)\n",
    "    .batch(config.batch_size)\n",
    "    .prefetch(config.AUTOTUNE)\n",
    ")\n",
    "with strategy.scope():\n",
    "  transformer_model = TFAutoModel.from_pretrained(config.hf_path)\n",
    "  model_a_one = create_model(transformer_model)\n",
    "model_a_one = create_model(transformer_model)\n",
    "hist = model_a_one.fit(train_dataset_onelang_a,steps_per_epoch= train_steps_onelang_a,validation_data= test_dataset_a, epochs = config.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDxRhW1-nWO7",
    "outputId": "830a98bd-64ec-4fbe-b0df-33c4f586fa5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 54s 2s/step\n"
     ]
    }
   ],
   "source": [
    "y_orig_onelang_a = model_a_one.predict(test_data_a , verbose = 1).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FG2Ll_omnb6Q",
    "outputId": "f14247a8-3c02-4015-c716-d84b35ba33f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Malayalam after training on Malayalam language\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-Abusive       0.63      0.79      0.70      2490\n",
      "     Abusive       0.45      0.27      0.34      1607\n",
      "\n",
      "    accuracy                           0.58      4097\n",
      "   macro avg       0.54      0.53      0.52      4097\n",
      "weighted avg       0.56      0.58      0.56      4097\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print('Classification report for Malayalam after training on Malayalam language')\n",
    "print(classification_report(y_orig_onelang_a, y_val_onelang_a, target_names=['Non-Abusive','Abusive']))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "NLP_MURIL_H2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0777313fb072423d84757dacb72e25fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9b303506a5045e2be0329a44d8bd74e",
      "placeholder": "​",
      "style": "IPY_MODEL_a5208b51eaf7452c90bc00031e2e8622",
      "value": "Downloading: 100%"
     }
    },
    "0adce5a1751e4e8888dd9063f38fa7ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_77b02d6768464879bf052c010d8bc099",
       "IPY_MODEL_d58ede2be6404684aaa9ac8cc646b288",
       "IPY_MODEL_2af651603e4744ff9871e17f892f91a6"
      ],
      "layout": "IPY_MODEL_512d140980954df984f5fa187daba712"
     }
    },
    "0bcb113152d94c28af8c1b85a29f5bd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "26588fdfa68542a1ae78b9db0bf7c24c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2935cfa3bfb948498c8fc73265df0673": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87bb9bf7ee094838bb404e8036f579b6",
      "max": 1299,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5806d405dff2493eb21f18e4deb27612",
      "value": 1299
     }
    },
    "2a68c737886c4bc69d77d94f865b491c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e2f899387e948adad7dac78ebd9347b",
      "placeholder": "​",
      "style": "IPY_MODEL_7f9f3a6342264b50828c7208a936ab2d",
      "value": "100%"
     }
    },
    "2af651603e4744ff9871e17f892f91a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f0926b2df7e349b6b08a9b787485c026",
      "placeholder": "​",
      "style": "IPY_MODEL_a9b3ee8b2a984fb9a8771b916cd639e1",
      "value": " 1.45G/1.45G [00:26&lt;00:00, 60.6MB/s]"
     }
    },
    "2e1b6ea4ce084b309c361dcdcd9b92d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32309479dbee4ba68f36435449a5a02f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2a68c737886c4bc69d77d94f865b491c",
       "IPY_MODEL_2935cfa3bfb948498c8fc73265df0673",
       "IPY_MODEL_c0373f9ead45402ea9b5c47cfb880bb9"
      ],
      "layout": "IPY_MODEL_8dc6ba706e31426b9cada96dc141b35f"
     }
    },
    "3e2f899387e948adad7dac78ebd9347b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4188ad3931e04d3ab2a08b98356aa39a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_66c8fc92eef742c8b17c392d72208a9d",
      "placeholder": "​",
      "style": "IPY_MODEL_26588fdfa68542a1ae78b9db0bf7c24c",
      "value": " 1.45G/1.45G [00:28&lt;00:00, 55.7MB/s]"
     }
    },
    "43bc3ba0dcf8488081e92f7296edd022": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0777313fb072423d84757dacb72e25fb",
       "IPY_MODEL_6af54ce01cd649d0a82699f13a3258e1",
       "IPY_MODEL_4188ad3931e04d3ab2a08b98356aa39a"
      ],
      "layout": "IPY_MODEL_5904b68e8d1d49f09f0de1a8a064d0ff"
     }
    },
    "512d140980954df984f5fa187daba712": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5806d405dff2493eb21f18e4deb27612": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5904b68e8d1d49f09f0de1a8a064d0ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66c8fc92eef742c8b17c392d72208a9d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6abf715d52a844a3a738e481d3eeda73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6af54ce01cd649d0a82699f13a3258e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e1b6ea4ce084b309c361dcdcd9b92d5",
      "max": 1558943652,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e33464a55d234493ae054048ee0e505d",
      "value": 1558943652
     }
    },
    "77b02d6768464879bf052c010d8bc099": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aac1575aa2084411ba026ccfa562ceaf",
      "placeholder": "​",
      "style": "IPY_MODEL_0bcb113152d94c28af8c1b85a29f5bd2",
      "value": "Downloading: 100%"
     }
    },
    "7f9f3a6342264b50828c7208a936ab2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "87bb9bf7ee094838bb404e8036f579b6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8dc6ba706e31426b9cada96dc141b35f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5208b51eaf7452c90bc00031e2e8622": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a9b3ee8b2a984fb9a8771b916cd639e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aac1575aa2084411ba026ccfa562ceaf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0373f9ead45402ea9b5c47cfb880bb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f7960384a4ab4a6dbcca7045a744d76c",
      "placeholder": "​",
      "style": "IPY_MODEL_f70adcdc7b134d008f9b0877a2241828",
      "value": " 1299/1299 [01:00&lt;00:00, 26.72it/s]"
     }
    },
    "d58ede2be6404684aaa9ac8cc646b288": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f287437bceb44a2d96c3ff0fbd5ac2f2",
      "max": 1558943652,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6abf715d52a844a3a738e481d3eeda73",
      "value": 1558943652
     }
    },
    "d9b303506a5045e2be0329a44d8bd74e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e33464a55d234493ae054048ee0e505d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f0926b2df7e349b6b08a9b787485c026": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f287437bceb44a2d96c3ff0fbd5ac2f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f70adcdc7b134d008f9b0877a2241828": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f7960384a4ab4a6dbcca7045a744d76c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
